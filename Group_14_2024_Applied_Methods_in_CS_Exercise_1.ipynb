{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "abh3akoro91w",
        "SGFMK9I_u42G"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/idanye/Applied-CS/blob/working-branch-Idan/Group_14_2024_Applied_Methods_in_CS_Exercise_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Waze & Dijkstra's Algorithm\n",
        "\n",
        "We saw in class how Waze utilizes Dijkstra's Algorithm as a fundamental component of its routing system. Dijkstra's Algorithm is a graph search algorithm that finds the shortest path between two nodes in a weighted graph. In the context of Waze, cities and roads are represented as nodes and edges in a graph, respectively. Each road segment is assigned a weight that reflects the estimated travel time, considering factors such as road type, traffic conditions, and speed limits. When a user inputs a starting and destination point, Waze's implementation of Dijkstra's Algorithm efficiently explores the road network to compute the shortest route. This algorithm ensures that users receive real-time, optimal directions that consider both distance and current traffic conditions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fyuhP5S8MsvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All imports\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "# Add more imports here if necessary:\n",
        "!pip install osmnx\n",
        "!pip install networkx\n",
        "!pip install datasets\n",
        "import osmnx as ox\n",
        "import networkx as nx\n",
        "import folium\n",
        "import heapq\n",
        "from datasets import load_dataset\n",
        "import string\n"
      ],
      "metadata": {
        "id": "tIEkHaa4ffP4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eedd63aa-dee9-4cbd-a3e7-dd0074cf4fcc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting osmnx\n",
            "  Downloading osmnx-1.8.1-py3-none-any.whl (102 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/102.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m102.4/102.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.8/102.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: geopandas>=0.12 in /usr/local/lib/python3.10/dist-packages (from osmnx) (0.13.2)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.10/dist-packages (from osmnx) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from osmnx) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.10/dist-packages (from osmnx) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.10/dist-packages (from osmnx) (2.31.0)\n",
            "Requirement already satisfied: shapely>=2.0 in /usr/local/lib/python3.10/dist-packages (from osmnx) (2.0.2)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.12->osmnx) (1.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.12->osmnx) (23.2)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.12->osmnx) (3.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->osmnx) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->osmnx) (2023.3.post1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->osmnx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->osmnx) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->osmnx) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->osmnx) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.12->osmnx) (23.1.0)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.12->osmnx) (8.1.7)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.12->osmnx) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.12->osmnx) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.12->osmnx) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.12->osmnx) (67.7.2)\n",
            "Installing collected packages: osmnx\n",
            "Successfully installed osmnx-1.8.1\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Collecting pyarrow-hotfix (from datasets)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Waze and other Graphs' Algorithms\n",
        "\n",
        "\n",
        "<img src=\"https://pentagram-production.imgix.net/b695e6d0-864c-49f7-8290-ea973d012d0e/LogotypeWazer_Lockups_02-04.jpg?rect=194%2C0%2C3603%2C2251&w=1500&fit=crop&fm=jpg&q=70&auto=format&h=935\" width=\"40%\">\n",
        "\n",
        "Do not start the exercise until you fully understand the submission guidelines, which can be found here.\n",
        "\n",
        "For any material-related-questions, ask Ami. For any organization-related-questions, ask Lior.\n",
        "\n",
        "\n",
        "#### **Read the following instructions carefully:**\n",
        "1. Write your functions in this notebook only. Do not create Python modules and import them.\n",
        "Feel free to add code blocks if you need.\n",
        "2. Answers to qualitative questions should be written in markdown cells (with  LATEX  support). Answers that will be written in commented code blocks will not be checked.\n",
        "3. Kind reminder: the total of all exercises weight is 50% of the course's grade!\n",
        "\n",
        "#### **This exercise summarizes the following subjects:**\n",
        "1. Waze (Dijkstra's Algorithm)\n",
        "2. Autocomplete Algorithm\n",
        "3. Huffman Coding\n",
        "4. Markov Chain"
      ],
      "metadata": {
        "id": "xA0HkKnTMj2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (5 points):\n",
        "1.1 Write a code that reads the `cities.csv` and `roads_matrix.csv` files into memory. You may choose the data structure by yourself.\n",
        "* `cities.csv` file contains `V=20` cities with columns \"City\", \"Latitude\", and \"Longitude\".\n",
        "* `roads_matrix.csv` file is a `V*V` matrix, where each cell contains a 0 (no road), a 1 (fast road) or a 2 (slow road).\n",
        "1.2 Write a function `create_map` that returns a map to visualize cities and roads.\n"
      ],
      "metadata": {
        "id": "abh3akoro91w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cities = \"cities.csv\"\n",
        "roads_matrix = \"roads_matrix.csv\""
      ],
      "metadata": {
        "id": "NSR5gVzJkTwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_map(cities, roads_matrix, show_roads=True):\n",
        "    \"\"\"\n",
        "    Creates a map to visualize cities and roads.\n",
        "    Hint: You can use the Folium library or any other library of your choice.\n",
        "    Args:\n",
        "    - cities (DataFrame): A DataFrame containing city data with columns 'City', 'Latitude', and 'Longitude'.\n",
        "    - roads_matrix (DataFrame): A DataFrame containing road data between cities.\n",
        "    - show_roads (bool): Whether to add roads to the map or not.\n",
        "\n",
        "    Returns:\n",
        "    - map: The map object.\n",
        "    \"\"\"\n",
        "    # Read the CSV files into pandas DataFrames\n",
        "    cities_df = pd.read_csv(cities)\n",
        "    roads_df = pd.read_csv(roads_matrix)\n",
        "\n",
        "    # Assuming the cities DataFrame has columns 'Latitude' and 'Longitude'\n",
        "    # Create a list of tuples for cities\n",
        "    cities = list(zip(cities_df['Latitude'], cities_df['Longitude']))\n",
        "\n",
        "    # Create a map centered around the first city\n",
        "    map_center = cities[0] if cities else (0, 0)\n",
        "    map_obj = folium.Map(location=map_center, zoom_start=5)\n",
        "\n",
        "    # Add markers for each city\n",
        "    for idx, (lat, lon) in enumerate(cities):\n",
        "        folium.Marker([lat, lon], tooltip=f'City {idx}').add_to(map_obj)\n",
        "\n",
        "    # Add lines for roads if show_roads is True\n",
        "    # Assuming roads DataFrame has columns 'StartCityIndex' and 'EndCityIndex'\n",
        "    if show_roads:\n",
        "        for _, row in roads_df.iterrows():\n",
        "            start_city = cities[row['StartCityIndex']]\n",
        "            end_city = cities[row['EndCityIndex']]\n",
        "            folium.PolyLine([start_city, end_city], color=\"blue\").add_to(map_obj)\n",
        "\n",
        "    return map_obj\n",
        "\n",
        "# Test your function:\n",
        "create_map(cities, roads_matrix, show_roads=True)"
      ],
      "metadata": {
        "id": "xkPiIdyTHmms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (15 points):\n",
        "Complete the class `RoadNetwork`. This class is responsible for initializing the road network, calculating distances between cities, and estimating travel times on different road types.\n",
        "\n",
        "Note: You can change the class structure however you see fit, this is just an example to help you get started."
      ],
      "metadata": {
        "id": "SGFMK9I_u42G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RoadNetwork:\n",
        "    def __init__(self, cities=cities, roads_matrix=roads_matrix):\n",
        "        pass\n",
        "\n",
        "    def calculate_distance(self, city1, city2):\n",
        "        \"\"\"\n",
        "        Calculates the distance (in kilometers) between two cities based on their coordinates.\n",
        "\n",
        "        Returns:\n",
        "        - distance_km (float): The distance between the two cities in kilometers.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def calculate_travel_time(self, source_city, destination_city, road_type):\n",
        "        \"\"\"\n",
        "        Calculates the time (in minutes) it takes to travel from source_city to destination_city, depending on road_type.\n",
        "\n",
        "        Args:\n",
        "        - source_city (str): The name of the source city.\n",
        "        - destination_city (str): The name of the destination city.\n",
        "        - road_type (int): The type of road (0, 1, or 2).\n",
        "\n",
        "        Returns:\n",
        "        - travel_time (float): The travel time in minutes.\n",
        "        \"\"\"\n",
        "        # Parameter p1 tells us how much time does it take to travel in road_type 1 in an avg pace of 25 KM/h\n",
        "        # Parameter p2 tells us how much time does it take to travel in road_type 2 in an avg pace of 10 KM/h\n",
        "        p1 = 1 / 25\n",
        "        p2 = 1 / 10\n",
        "\n",
        "        # We use parameters r1 and r2 to multiply the L2 distance, to get closer to a real-world road distance between the cities.\n",
        "        r1 = 2\n",
        "        r2 = 1.5\n",
        "\n",
        "        pass"
      ],
      "metadata": {
        "id": "4gRNE2wseomo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "95ba5389-7364-4752-e0b0-0ab5c4a8bad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cities' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0f546708f8f5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRoadNetwork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroads_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroads_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-0f546708f8f5>\u001b[0m in \u001b[0;36mRoadNetwork\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mRoadNetwork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroads_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroads_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cities' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 (20 points):\n",
        "3.1 Write your own Dijkstra's Algorithm to compute the shortest path between two cities.\n",
        "\n",
        "3.2 Write a function `find_shortest_path` that performs the following tasks:\n",
        "\n",
        "* Calculates and prints the estimated time of arrival (ETA) for the shortest path.\n",
        "* Prints the detailed path from the starting city to the destination city.\n",
        "* Optionally, displays the route on a map object for better visualization.\n",
        "\n",
        "3.3 Test your function:\n",
        "*   Find and **visualize** the shortest path from **Ashdod** to **Herzeliya**.\n",
        "*   Find and **visualize** the shortest path from **Netanya** to **Tel Aviv**.\n",
        "*   Choose any two cities with a minimum distance of 30 kilometers between them, and find and **visualize** the shortest path between them.\n",
        "\n"
      ],
      "metadata": {
        "id": "rIkSaSf2xZe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dijkstra(graph, start, end):\n",
        "    \"\"\"\n",
        "    Finds the shortest path between two cities using Dijkstra's algorithm.\n",
        "\n",
        "    Args:\n",
        "    - graph: The road network object.\n",
        "    - start: The starting city.\n",
        "    - end: The destination city.\n",
        "\n",
        "    Returns:\n",
        "    - path (list): The shortest path as a list of city names.\n",
        "    - times (dict): The travel times to each city on the path.\n",
        "    \"\"\"\n",
        "    # Priority queue to hold vertices and their distances from the start\n",
        "    queue = [(0, start)]\n",
        "    times = {vertex: float('infinity') for vertex in graph}\n",
        "    times[start] = 0  # Distance from start to start is obviously 0\n",
        "    predecessors = {vertex: None for vertex in graph}\n",
        "\n",
        "    while queue:\n",
        "        current_distance, current_vertex = heapq.heappop(queue)\n",
        "\n",
        "        # If we've reached the end, break out of the loop\n",
        "        if current_vertex == end:\n",
        "            break\n",
        "\n",
        "        # If the dequeued distance is greater than the already found shortest distance, skip\n",
        "        if current_distance > times[current_vertex]:\n",
        "            continue\n",
        "\n",
        "        for neighbor, weight in graph[current_vertex].items():\n",
        "            distance = current_distance + weight\n",
        "\n",
        "            # If a shorter path to the neighbor is found\n",
        "            if distance < times[neighbor]:\n",
        "                times[neighbor] = distance\n",
        "                predecessors[neighbor] = current_vertex\n",
        "                heapq.heappush(queue, (distance, neighbor))\n",
        "\n",
        "    # Reconstruct path from end to start by predecessors\n",
        "    path = []\n",
        "    current = end\n",
        "    while current is not None:\n",
        "        path.insert(0, current)\n",
        "        current = predecessors[current]\n",
        "\n",
        "    return path, times\n",
        "\n",
        "\n",
        "def find_shortest_path(road_network, start_city, end_city, show_on_map=False):\n",
        "    \"\"\"\n",
        "    Prints the estimated time of arrival (ETA),\n",
        "    the path from the starting city to the destination,\n",
        "    and optionally displays the route on a map.\n",
        "\n",
        "    Args:\n",
        "    - start_city: The starting city.\n",
        "    - end_city: The destination city.\n",
        "    - show_on_map (bool): Whether to display the route on a map.\n",
        "\n",
        "    Returns:\n",
        "    - map object or None: The map object if show_on_map is True, otherwise None.\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "sHPZEpV_B6D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function\n",
        "# Dummy graph representation\n",
        "# Graph is represented as a dictionary of dictionaries\n",
        "# Outer dictionary represents each vertex and its neighbors\n",
        "# Inner dictionary represents neighbors and their respective distances\n",
        "dummy_graph = {\n",
        "    \"A\": {\"B\": 1, \"C\": 4},\n",
        "    \"B\": {\"A\": 1, \"C\": 2, \"D\": 5},\n",
        "    \"C\": {\"A\": 4, \"B\": 2, \"D\": 1},\n",
        "    \"D\": {\"B\": 5, \"C\": 1}\n",
        "}\n",
        "\n",
        "# Test the function with the dummy graph\n",
        "path, times = dijkstra(dummy_graph, \"A\", \"D\")\n",
        "path, times"
      ],
      "metadata": {
        "id": "aGxItkbiRQPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fdef151-0d28-4cd3-f0e5-b12d4d078ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['A', 'B', 'C', 'D'], {'A': 0, 'B': 1, 'C': 3, 'D': 4})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Autocomplete\n",
        "\n",
        "In this section, we will explore the autocomplete algorithm. Autocomplete is a feature commonly used in applications, providing word or phrase suggestions as users type. We will be working on implementing this functionality using a data structure called a Trie, as seen on class.\n",
        "\n",
        "To get started, we will load a dataset that will serve as the basis for your autocomplete system. We will use the Hugging Face `datasets` library. **Hugging Face** is a leading platform for NLP and Machine Learning that provides access to state-of-the-art models, datasets, and tools for developing and deploying applications in the field of natural language understanding. Familiarizing yourself with this platform is valuable if you're interested in NLP and ML."
      ],
      "metadata": {
        "id": "fTnck6BljvdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Before we can load the dataset, we need to install the `datasets` library. The following command will install it: `!pip install datasets -q`"
      ],
      "metadata": {
        "id": "L_mdOgahev9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install the necessary library\n",
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "1xY3_EcdeybK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import and load the dataset\n",
        "import datasets"
      ],
      "metadata": {
        "id": "4gguwoGx9AK9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 (5 points):\n",
        "\n",
        "Write a function `read_data` to read and preprocess the data.\n",
        "\n",
        "We'll use a dataset called `google_wellformed_query`. Google's query wellformedness dataset was created by crowdsourcing well-formedness annotations for 25,100 queries from the Paralex corpus. Each query in this dataset has been annotated by five raters, each providing a rating of 1 or 0 to indicate whether or not the query is well-formed. A rating of 1 suggests that the query is well-formed, while a rating of 0 suggests otherwise.\n",
        "\n",
        "Note: For this assignment, you only need to work with the `train` split of the dataset."
      ],
      "metadata": {
        "id": "1FpmhpNAez6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(dataset, rate):\n",
        "    \"\"\"\n",
        "    This function reads data from a dataset and filters it based on a specified rating threshold.\n",
        "\n",
        "    Args:\n",
        "        dataset (dict): The dataset to read from.\n",
        "        rate (float): The rating threshold. Sentences with a rating greater than or equal to this threshold will be included in the output.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of the sentences.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "\n",
        "    for data in dataset:\n",
        "        if data['rating'] >= rate:\n",
        "            sentences.append(data['content'])\n",
        "\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "l8FT4PnLfJfd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the data using the dataset library and your function here:\n",
        "dataset_train = load_dataset(\"google_wellformed_query\", split=\"train\")\n",
        "# dataset_train\n",
        "sentences = read_data(dataset_train, 0)"
      ],
      "metadata": {
        "id": "cnbdZ0MYjc_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2937e73-cb6b-425a-8d8a-440f7dcb1e97"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5 (15 points):\n",
        "\n",
        "Complete the Autocomplete function we saw in class, which provides automatic word completions as follows:\n",
        "\n",
        "*   The function should accept user input as a complete sentence.\n",
        "*   It will simulate word-by-word printing.\n",
        "*   Suggestions will be based on probability calculations within a Trie data structure.\n",
        "*   Probabilities are determined by counters, representing word occurrences in the Trie.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "User types **\"How do you make the perfect pizza?\"**, your function returns the following:\n",
        "*   **How** many calories in a cup of bluberries\n",
        "*   **How do** you change the alternator belt on a 1998 audi a4\n",
        "*   **How do you** make the perfect pizza?\n",
        "*   **How do you make** the perfect pizza?\n",
        "*   **How do you make the** perfect pizza?\n",
        "*   **How do you make the perfect** pizza?\n",
        "*   **How do you make the perfect pizza?**\n",
        "\n",
        "**Note:** This is an example based on a specific dataset. Changing or expanding the dataset could potentially yield better results.\n",
        "\n",
        "**Feel free to make changes and adjustments to the code.**"
      ],
      "metadata": {
        "id": "0mTFeUnHiYKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrieNode(object):\n",
        "    \"\"\"\n",
        "    Our trie node implementation. Very basic, but does the job.\n",
        "      word: The word associated with the node.\n",
        "      children: A list of child nodes.\n",
        "      sentence_finished: A boolean indicating if the current node marks the end of a sentence.\n",
        "      counter: A count of how many times the word associated with this node has appeared in the addition process.\n",
        "    \"\"\"\n",
        "    def __init__(self, word):\n",
        "        self.word = word\n",
        "        self.children = {}\n",
        "        # Is it the last word of the sentence.\n",
        "        self.sentence_finished = False\n",
        "        # How many times this word appeared in the addition process\n",
        "        self.counter = 1\n",
        "\n",
        "class Trie():\n",
        "    def __init__(self, text):\n",
        "        \"\"\"\n",
        "        Init a Trie with the given sentences list.\n",
        "        During initialization, it removes punctuation, converts sentences to lowercase, and adds them to the Trie using the add method.\n",
        "        \"\"\"\n",
        "        self.root = TrieNode('*')\n",
        "        self.text = text\n",
        "        self.punctuation = set(string.punctuation)\n",
        "        for sentence in self.text:\n",
        "            sentence = ''.join(char for char in sentence if char not in self.punctuation)\n",
        "            self.add(sentence.lower())\n",
        "\n",
        "    def add(self, sentence):\n",
        "        \"\"\"\n",
        "        This method adds a sentence to the Trie structure.\n",
        "        It breaks the sentence into words, and for each word, it traverses the Trie, adding new nodes as necessary or updating existing nodes.\n",
        "        \"\"\"\n",
        "        node = self.root\n",
        "        for word in sentence.split():\n",
        "            if word not in node.children:\n",
        "                new_node = TrieNode(word)\n",
        "                node.children[word] = new_node\n",
        "            node = node.children[word]\n",
        "            node.counter += 1\n",
        "        node.sentence_finished = True\n",
        "\n",
        "    def find_prefix(self, prefix):\n",
        "        \"\"\"\n",
        "        Check and return\n",
        "          1. If the prefix exsists in any of the words we added so far\n",
        "          2. If yes then how many words actually have the prefix\n",
        "          3. The node where the prefix ends in the Trie structure\n",
        "        \"\"\"\n",
        "        node = self.root\n",
        "        # If the root node has no children, then return False.\n",
        "        # Because it means we are trying to search in an empty trie\n",
        "        if not self.root.children:\n",
        "            return False, 0, node\n",
        "\n",
        "        for word in prefix.split():\n",
        "            if word in node.children:\n",
        "                node = node.children[word]\n",
        "            else:\n",
        "                return False, 0, node\n",
        "        return True, node.counter, node\n",
        "\n",
        "    # def list_all_children(self, prefix):\n",
        "    #     \"\"\"\n",
        "    #     Returns all the children of the node at the end of a prefix, with the counter for each child\n",
        "    #     \"\"\"\n",
        "    #     auto_complete_sentence = prefix\n",
        "    #     (exists, counter, node) = self.find_prefix(auto_complete_sentence)\n",
        "    #     print(f'This prefix occurs {counter} times.')\n",
        "\n",
        "    #     for child in node.children:\n",
        "    #         print(f\"'{child.word}' appears {child.counter} time(s).\")\n",
        "\n",
        "    #     return\n",
        "\n",
        "    def suggestions_rec(self, node, word):\n",
        "        \"\"\"\n",
        "        Recursive function to collect the end of all words below the given node.\n",
        "        \"\"\"\n",
        "        if node.sentence_finished:\n",
        "            sentence_list.append((path.strip(), node.counter))\n",
        "\n",
        "        for child_node in node.children:\n",
        "            new_path = path + ' ' + child_node.word if path else child_node.word\n",
        "            self.suggestions_rec(child_node, new_path, sentence_list)\n",
        "\n",
        "    def auto_complete(self, prefix):\n",
        "        \"\"\"\n",
        "        Suggests the most probable word completion for a given prefix.\n",
        "\n",
        "        Args:\n",
        "        - prefix (str): The input prefix to autocomplete.\n",
        "\n",
        "        Returns:\n",
        "        - suggestion (str): The suggested word completion.\n",
        "        \"\"\"\n",
        "        exists, _, node = self.find_prefix(prefix)\n",
        "\n",
        "        if not exists:\n",
        "            return []\n",
        "\n",
        "        sentence_list = []\n",
        "        self.suggestions_rec(node, '', sentence_list)\n",
        "        sentence_list.sort(key=lambda x: (-x[1], x[0]))\n",
        "\n",
        "        return [sentence for sentence, _ in sentence_list]\n",
        "\n",
        "\n",
        "    def init_auto_complete(self):\n",
        "        \"\"\"\n",
        "        Initializes the auto-complete system and allows the user to interactively\n",
        "        input prefixes and get suggestions.\n",
        "\n",
        "        \"\"\"\n",
        "        prefix = input(\"Enter a prefix: \")\n",
        "        suggestions = self.auto_complete(prefix)\n",
        "\n",
        "        if suggestions:\n",
        "            print(\"Suggestions:\")\n",
        "\n",
        "            for suggestion in suggestions:\n",
        "                print(suggestion)\n",
        "        else:\n",
        "            print(\"No suggestions available for this prefix.\")\n"
      ],
      "metadata": {
        "id": "ebj_uxg87Z5G"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example:\n",
        "t = Trie(sentences)\n",
        "t.init_auto_complete()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwOObMUdH2al",
        "outputId": "1f12fd09-49eb-477e-c1b7-32754fa0637c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a prefix: How do you make the perfect pizza?\n",
            "No suggestions available for this prefix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Huffman Coding\n",
        "\n",
        "In these questions, we'll utilize a full English text to create frequency tables for the English language.\n",
        "\n",
        "We will use a book text to calculate frequencies of all English alphabet letters, including spaces.\n",
        "\n",
        "Read the book provided through Moodle using the `read_text` function."
      ],
      "metadata": {
        "id": "sNfP7gluj06T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_text(file):\n",
        "    f = open(file, 'r', encoding='utf-8')\n",
        "    text0 = f.read()\n",
        "    f.close()\n",
        "\n",
        "    text0 = text0.lower()\n",
        "\n",
        "    legalABC = 'abcdefghijklmnopqrstuvwxyz '\n",
        "    text = ''\n",
        "    last_was_space = True\n",
        "\n",
        "    for ch in text0:\n",
        "        if ch == '\\n':\n",
        "            ch = ' '\n",
        "        if ch in legalABC:\n",
        "            if ch != \" \":\n",
        "                text = text + ch\n",
        "                last_was_space = False\n",
        "            if ch == \" \" and last_was_space == False:\n",
        "                text= text + ch\n",
        "                last_was_space = True\n",
        "    return text\n",
        "\n",
        "text = read_text('book-hw1.txt')"
      ],
      "metadata": {
        "id": "rGj57JNcxlL3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "4995b740-5d4e-468c-c434-814caa158fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-19c0bbe7ec37>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'book-hw1.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-19c0bbe7ec37>\u001b[0m in \u001b[0;36mread_text\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtext0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'book-hw1.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6 (10 points):\n",
        "\n",
        "Create the frequency table and print it.\n",
        "\n",
        "Count only lowercase letters '**a**' through '**z**' and the **space** character.\n",
        "\n",
        "**Store frequencies as percentages (like 0.8% or 0.008), not their count.**\n",
        "\n",
        "Example: {' ': 17, 'a': 8.1, 'b': 1.4, ..., 'z': 0.07 }"
      ],
      "metadata": {
        "id": "s1ujO3sCdA1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_table(text):\n",
        "    \"\"\"\n",
        "    Calculate the frequency of each character in the text as a percentage of the total characters.\n",
        "    Sort the results alphabetically.\n",
        "    Args:\n",
        "      text (str): your text file.\n",
        "\n",
        "    Returns:\n",
        "      freq_table (dict): A dictionary containing characters as keys and their corresponding frequencies as values.\n",
        "                         The dictionary is sorted alphabetically by character.\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "zn0Ry4jD0j2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print your table here"
      ],
      "metadata": {
        "id": "A192f9-8vqlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following is a table frequency of general English text that includes only the letters '**a**' through '**z**', make sure that your table looks something like this:\n",
        "\n",
        "Letter | Frequency\n",
        "--- | :---:\n",
        "SPACE |\n",
        "a | 8.167%\n",
        "b | 1.492%\n",
        "c | 2.782%\n",
        "d | 4.253%\n",
        "e | 12.702%\n",
        "f | 2.228%\n",
        "g | 2.015%\n",
        "h | 6.094%\n",
        "i | 6.966%\n",
        "j | 0.253%\n",
        "k | 1.772%\n",
        "l | 4.025%\n",
        "m | 2.406%\n",
        "n | 6.749%\n",
        "o | 7.507%\n",
        "p | 1.929%\n",
        "q | 0.095%\n",
        "r | 5.987%\n",
        "s | 6.327%\n",
        "t | 9.056%\n",
        "u | 2.758%\n",
        "v | 0.978%\n",
        "w | 2.36%\n",
        "x | 0.25%\n",
        "y | 1.974%\n",
        "z | 0.074%"
      ],
      "metadata": {
        "id": "n075fu-XtbG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7 (10 points):\n",
        "\n",
        "Write code to construct a Huffman Tree representing English alphabet letters, including spaces.\n",
        "\n",
        "The goal of this exercise is to create an encoding table that looks something like:\n",
        "\n",
        "a : 0000\n",
        "\n",
        "b : 00010\n",
        "\n",
        "c : 00011\n",
        "\n",
        "...\n",
        "\n",
        "**Feel free to make changes and adjustments to the code.**"
      ],
      "metadata": {
        "id": "LUCc63AQdr_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NodeTree(object):\n",
        "    \"\"\"\n",
        "    Represents a node in the Huffman Tree.\n",
        "\n",
        "    Args:\n",
        "    left (NodeTree): The left child node (default is None).\n",
        "    right (NodeTree): The right child node (default is None).\n",
        "\n",
        "    Methods:\n",
        "    children(): Returns the left and right children of the node.\n",
        "    str(): Returns a string representation of the node, including its left and right children.\n",
        "    \"\"\"\n",
        "    def __init__(self, left=None, right=None):\n",
        "        pass\n",
        "\n",
        "    def children(self):\n",
        "        pass\n",
        "\n",
        "    def __str__(self):\n",
        "        pass\n",
        "\n",
        "def huffman_code_tree(node, binString=''):\n",
        "    \"\"\"\n",
        "    Recursively generates Huffman codes for characters in the Huffman Tree.\n",
        "\n",
        "    Args:\n",
        "    node (NodeTree): The current node in the Huffman Tree.\n",
        "    binString (str, optional): The binary string for recursion.\n",
        "\n",
        "    Returns:\n",
        "    codes (dict): A dictionary of characters and their corresponding Huffman codes.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def make_tree(nodes):\n",
        "    \"\"\"\n",
        "    Constructs the Huffman Tree from a list of nodes representing characters and their frequencies.\n",
        "\n",
        "    Args:\n",
        "    nodes (list): A list of tuples with character-frequency pairs.\n",
        "\n",
        "    Returns:\n",
        "    root (NodeTree): The root node of the Huffman Tree.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def init_huffman(text, print_code=False):\n",
        "    \"\"\"\n",
        "    Initializes Huffman encoding for a given text.\n",
        "\n",
        "    Args:\n",
        "    text (str): The input text to be encoded.\n",
        "    print_code (bool, optional): Whether to print Huffman codes.\n",
        "\n",
        "    Returns:\n",
        "    encoding (dict): A dictionary of characters and their Huffman codes.\n",
        "    freq (list): A sorted list of tuples representing character frequencies.\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "hFMxatOHdxuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8 (5 points):\n",
        "* Encode the first 1000 letters from our text using Huffman coding and print your encoding table.\n",
        "Determine the number of bits required for Huffman encoding.\n",
        "Compare this to storing letters in ASCII code, which needs 8 bits per character (totaling 8000 bits for 1000 characters).\n",
        "\n",
        "* Encode this message in Huffman code: **\"The rat and the bear had a date. The rat had a bad hat. The bear had a red beard. Then, the rat had tea and the bear had naan.\"** Print your encoding table. Determine the number of bits required for Huffman encoding.\n",
        "Compare this to storing letters in ASCII code, which needs 8 bits per character.\n",
        "\n",
        "* Answer the open question below.\n",
        "\n",
        "* **Note:** Don't forget to remove punctuations and use only lowercased letters."
      ],
      "metadata": {
        "id": "S-l9hEsndyeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pass"
      ],
      "metadata": {
        "id": "mmEvDXM1d260"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explain in your own words why using Huffman coding is more efficient in the second encoding of the short sentence compared to encoding the entire text."
      ],
      "metadata": {
        "id": "u7gkl9jCLtWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANWER GOES HERE"
      ],
      "metadata": {
        "id": "25iY48A_LvsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9 (5 points):\n",
        "Here is a list of words:\n",
        "\n",
        "`Hi, For, Forest, False, Falls, Fall, Friend, Friends, High`\n",
        "\n",
        "Create binary representations using the Huffman code method.\n",
        "\n",
        "Build the Huffman Tree for encoding and provide the binary representation for the message \"False Friends Fall High.\"\n",
        "\n",
        "Use separators between words for clarity."
      ],
      "metadata": {
        "id": "rsS9o7nKd30S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pass"
      ],
      "metadata": {
        "id": "uCro5JCIfYMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Markov Chain\n",
        "\n",
        "In this exercise, you will work with Markov Chains to generate text based on probabilistic patterns."
      ],
      "metadata": {
        "id": "FnVYZnYbkATN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10 (15 points):\n",
        "\n",
        "1.   **Load and Tokenize:** Begin by loading the `Brown` corpus from the `NLTK` library and tokenize it into words using `word_tokenize`. The `Brown` corpus will serve as our source text.\n",
        "\n",
        "2.   **Build N-gram Models:** Create various n-gram models (e.g., unigrams, bigrams) to capture word sequences from the tokenized text. While building these models, calculate transition probabilities between n-grams.\n",
        "\n",
        "3.   **Generate Text:** Implement a text generation function that uses the calculated probabilities. This function should generate text based on the patterns observed in the corpus.\n",
        "\n",
        "4.   **Print Results:** For each n-gram order (1, 2, 3, and 4-gram), print the generated text. You can inspect the results to understand how different n-gram orders affect the generated text.\n",
        "\n",
        "**Example of Transition Probabilities:**\n",
        "\n",
        "For a 2-gram, the probabilities might look like this:\n",
        "\n",
        "```\n",
        "{('to', 'wait'): {'one': 0.043478260869565216,\n",
        "  'until': 0.21739130434782608,\n",
        "  ',': 0.21739130434782608,\n",
        "  'for': 0.17391304347826086,\n",
        "  'a': 0.043478260869565216,\n",
        "  'his': 0.043478260869565216,\n",
        "  '.': 0.08695652173913043,\n",
        "  'to': 0.043478260869565216,\n",
        "  'till': 0.08695652173913043,\n",
        "  'before': 0.043478260869565216},...}\n",
        "```"
      ],
      "metadata": {
        "id": "GwVhrAfLf7_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import brown"
      ],
      "metadata": {
        "id": "ERdn7xKrkINu",
        "outputId": "28869122-93db-4075-c78b-7d0e3e7141c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for 42\n",
        "SEED = 42\n",
        "random.seed(SEED)"
      ],
      "metadata": {
        "id": "yvjMUqzaDGh8",
        "outputId": "86ca993c-fb88-464e-9717-e5f96f2f6a19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-556bcb0b646f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set the random seed for 42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mSEED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text corpus\n",
        "corpus = None\n",
        "tokens = None"
      ],
      "metadata": {
        "id": "0upZNX0oDIE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ngram_model(tokens, n):\n",
        "    \"\"\"\n",
        "    Build an n-gram model from a list of tokens.\n",
        "\n",
        "    Args:\n",
        "    - tokens (list): List of tokens from the corpus.\n",
        "    - n (int): The order of the n-grams to build (e.g., 1 for unigrams, 2 for bigrams, 3 for trigrams).\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary containing n-grams as keys and their associated probability distributions as values.\n",
        "    \"\"\"\n",
        "    ngrams = {}\n",
        "    pass"
      ],
      "metadata": {
        "id": "ghRyIGIJDLDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(ngrams, length):\n",
        "    \"\"\"\n",
        "    Generate text using an n-gram model.\n",
        "\n",
        "    Args:\n",
        "    - ngrams (dict): An n-gram model dictionary generated by build_ngram_model.\n",
        "    - length (int): The desired length of the generated text in tokens.\n",
        "\n",
        "    Returns:\n",
        "    - str: The generated text as a string.\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "3PzxWRSMDQnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build n-gram models of different orders with probabilities\n",
        "n1_gram = build_ngram_model(tokens, n=1)\n",
        "n2_gram = build_ngram_model(tokens, n=2)\n",
        "n3_gram = build_ngram_model(tokens, n=3)\n",
        "n4_gram = build_ngram_model(tokens, n=4)"
      ],
      "metadata": {
        "id": "y-XTBnyJDTHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text using the n-gram models with transition probabilities\n",
        "n1_generated_text = generate_text(n1_gram, length=10)\n",
        "n2_generated_text = generate_text(n2_gram, length=10)\n",
        "n3_generated_text = generate_text(n3_gram, length=10)\n",
        "n4_generated_text = generate_text(n4_gram, length=10)\n",
        "\n",
        "# Print the generated text\n",
        "print(f'1-Gram Generated Text: {n1_generated_text}')\n",
        "print(f'2-Gram Generated Text: {n2_generated_text}')\n",
        "print(f'3-Gram Generated Text: {n3_generated_text}')\n",
        "print(f'4-Gram Generated Text: {n4_generated_text}')"
      ],
      "metadata": {
        "id": "lmE_hfabDVPT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}